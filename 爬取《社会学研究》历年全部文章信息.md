# ggstudyttup
python/data analysis/statistics
import requests
import bs4
import re
import numpy
from bs4 import BeautifulSoup

def gethtml(url):#获取网页内容
        try:
           r=requests.get(url)
           r.raise_for_status()
           r.encoding=r.apparent_encoding
           return r.text
        except:
           return ""  

def get_id(articul_id,html):#获取文章id
    try:
        soup=BeautifulSoup(html,"html.parser")
        idlinks=soup("a") 
        for ids in idlinks:
            if str(ids.get("href"))[:24]=="http://oa.shxyj.org/Stat":
                href=str(ids.get("href"))
                articul_id.append(re.findall(r"(?<==)[1-9]\d{0,3}|10000(?=&)",href))#正则表达式
        return(articul_id)
    except:
        return "获取文章id失败"
    
def get_info(articul_infos,articul_id,count):#获取文章信息
    try:
        
        for ids in articul_id:
            
            count+=1
            
            url="http://www.shxyj.org/Magazine/show/?id="+ids[0]#获取每一篇文章的地址
            html=gethtml(url)
            soup=BeautifulSoup(html,"html.parser")
            name=soup.title.get_text()#获取文章名
            link=soup.find_all("a")
            
            url=[]
            
            for i in link:
                if str(i.get("href"))[:24]=="http://oa.shxyj.org/Stat":
                    url.append(i.get("href"))#获取文章链接：一个在线，一个下载
                   
            
            authorinfo=soup.find_all("td")#作者信息

            info_lists=[]
            
            for info in authorinfo:
                info_lists.append(info.get_text())
            

            for i in range(11):
                info_lists.pop(len(info_lists)-1)#删除末尾无关信息
            

            info_lists.insert(0,name)#将文章名添加进列表

            articul_info=[count]+info_lists+url
            articul_infos.append(articul_info)
        
        return(articul_infos)
    except:
        return "获取文章信息失败"

    
def into_file():#将爬到的文章信息写入excel
    try:
        
        return""
    
    except:
        return"写入文件失败"

def main():
    
    articul_id=[]
    articul_info=[]
    articul_infos=[]
    count=0
    pages=range(1,3)
    
    for page in pages:
        
        url="http://www.shxyj.org/Magazine?Year=&Issue=&Title=&Keywords=&WorkUnit=&page="+str(page)
        
        
        html=gethtml(url)
        
        articul_id=get_id(articul_id,html)
    
    articul_infos=get_info(articul_infos,articul_id,count)
        
    print(articul_infos)
    
       
main()
